{"searchDocs":[{"title":"üëê Construct Search queries","type":0,"sectionRef":"#","url":"/search-lab/docs/aggregations/search-stage","content":"","keywords":"","version":"Next"},{"title":"Aggregations in the Atlas UI‚Äã","type":1,"pageTitle":"üëê Construct Search queries","url":"/search-lab/docs/aggregations/search-stage#aggregations-in-the-atlas-ui","content":" Navigate to the Collections tab of your database deployment, pick the books collection, and navigate to the Aggregation tab from the navbar under your collection details.    tip The Atlas UI can start feeling a bit cramped at this point. You can also use the aggregation pipeline builder in Compass for a better experience.  Click the Add Stage button and type $search in the select input.    Add the following code for the $search stage.  { index: &quot;fulltextsearch&quot;, text: { query: &quot;cooking&quot;, path: [&quot;title&quot;] } }   The stage uses the &quot;fulltextsearch&quot; index. You don't need to explicitly define the index if it's &quot;default&quot; but you can keep it for clarity.  The text operator will search for &quot;cooking&quot; in the title field. You should see a collection of documents returned on the right.  Click the Add Stage button, scroll down, and select $project for Stage 2.    Add the following implementation for the $project stage to filter the returned fields.  { title: 1, authors: 1, genres: 1, pages: 1 }   At this point, you should see similar results as what you did in the last section, as this is pretty much the same query. ","version":"Next","tagName":"h2"},{"title":"üëê Add full-text search to your application","type":0,"sectionRef":"#","url":"/search-lab/docs/add-to-app/add-to-app","content":"","keywords":"","version":"Next"},{"title":"Using aggregation pipelines in Node.js‚Äã","type":1,"pageTitle":"üëê Add full-text search to your application","url":"/search-lab/docs/add-to-app/add-to-app#using-aggregation-pipelines-in-nodejs","content":" To use the aggregation pipeline in Node.js, you will need to use the aggregate method on the collection object. This method takes an array of stages as an argument, and returns a cursor. You can then use the cursor to iterate over the results, or use the toArray method to get the results in an array.  const documents = await collection.aggregate(aggregationPipelines).toArray();   You now know everything you need to add full-text search capabilities to your application.  ","version":"Next","tagName":"h2"},{"title":"Adding search to the library app‚Äã","type":1,"pageTitle":"üëê Add full-text search to your application","url":"/search-lab/docs/add-to-app/add-to-app#adding-search-to-the-library-app","content":" Open up the code from the server file server/src/controllers/books.ts. In there, look for the searchBooks function.  Right now, it uses a regular expression to query the database.  server/src/controllers/books.ts public async searchBooks(query: string): Promise&lt;Book[]&gt; { const books = await collections?.books?.find({ title: {$regex: new RegExp(query, &quot;i&quot;)} }).toArray(); return books; }   While this code works to a certain extent, it is less than optimal. As the dataset grows, the performance of this query will degrade because it will have to scan the entire collection. You cannot query the index with a regular expression. Furthermore, the query only matches on the title, and only for the exact sequence of characters.  Change this code to use the search index instead. You will need to use the $search stage in the aggregation pipeline. Have your search cover the title, the author name, and the genres array.  This code will go in the server/src/controllers/books.ts file, in the searchBooks function.  Click here to see the answer server/src/controllers/books.ts public async searchBooks(query: string): Promise&lt;Book[]&gt; { const aggregationPipeline = [ { $search: { index: 'fulltextsearch', text: { query, path: ['title', 'authors.name', 'genres'] } } } ]; const books = await collections?.books?.aggregate(aggregationPipeline).toArray() as Book[]; return books; }   Play around the application, and look at what are the results you're getting. You should see that the results are more relevant than before. ","version":"Next","tagName":"h2"},{"title":"ü¶∏ Create an index","type":0,"sectionRef":"#","url":"/search-lab/docs/facet/create-index","content":"ü¶∏ Create an index In order to use faceting, you will need to create the appropriate index. Create a new search index using the JSON editor. Use the following JSON for your index definition. { &quot;mappings&quot;: { &quot;dynamic&quot;: false, &quot;fields&quot;: { &quot;genres&quot;: { &quot;type&quot;: &quot;stringFacet&quot; }, &quot;year&quot;: { &quot;type&quot;: &quot;number&quot; } } } } Proceed like you did in the previous exercises to create a new index. Once the index is ready, you can proceed to the next step.","keywords":"","version":"Next"},{"title":"ü¶∏ Intro to facets","type":0,"sectionRef":"#","url":"/search-lab/docs/facet/intro","content":"ü¶∏ Intro to facets A facet is a way to group documents together based on a common value. For example, if you have a list of movies, you might want to group them by genre. You could then use the facet to filter the results to only show movies of a certain genre. A common use case is to use facets to create a filter. You can see this in most e-commerce sites. For example, you will see a list of categories on the left side of the page, and each category will have a number next to it. This number represents the number of products in that category. { &quot;facet&quot;: { &quot;operator&quot;: { &quot;range&quot;: { &quot;path&quot;: &quot;year&quot;, &quot;gte&quot;: 2000, &quot;lte&quot;: 2015 } }, &quot;facets&quot;: { &quot;genresFacet&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;path&quot;: &quot;genres&quot; } } } } ","keywords":"","version":"Next"},{"title":"üìò Full Text Search","type":0,"sectionRef":"#","url":"/search-lab/docs/full-text-search","content":"","keywords":"","version":"Next"},{"title":"What is Atlas Search?‚Äã","type":1,"pageTitle":"üìò Full Text Search","url":"/search-lab/docs/full-text-search#what-is-atlas-search","content":" Atlas Search integrates with the other Atlas services to provide you with a fully managed search service. It's built on top of Lucene, the leading open-source search engine library. It's also built on top of MongoDB's aggregation framework, which means that you can use the same query language that you use to query your data to search your data.  ","version":"Next","tagName":"h2"},{"title":"Benefits of Atlas Search‚Äã","type":1,"pageTitle":"üìò Full Text Search","url":"/search-lab/docs/full-text-search#benefits-of-atlas-search","content":" You've seen the limitations of querying your data using the MongoDB query language. Atlas Search provides you with several benefits:  Full-text search - Atlas Search provides you with full-text search capabilities, including stemming, stop words, and synonyms.Scoring - Atlas Search provides you with a scoring system that allows you to rank your search results based on relevance.Language support - Atlas Search supports multiple languages out of the box.Autocomplete - Atlas Search provides you with autocomplete capabilities.Highlighting - Atlas Search provides you with highlighting capabilities. ","version":"Next","tagName":"h2"},{"title":"ü¶∏ Query with facets","type":0,"sectionRef":"#","url":"/search-lab/docs/facet/query","content":"ü¶∏ Query with facets Now that your index is in place, you can query using it. You will need to use the $searchMeta aggregation stage to query your data. You can find more about this stage, and how to use it in the Atlas Search documentation. Now that you have an index on the genres and year fields, try creating a query that returns the number of books in each genre, for books published in the 2000's. Click to see the answer { $searchMeta: { &quot;index&quot;: &quot;facetsIndexName&quot;, &quot;facet&quot;: { &quot;operator&quot;: { &quot;range&quot;: { &quot;path&quot;: &quot;year&quot;, &quot;gte&quot;: 2000, &quot;lte&quot;: 2010 } }, &quot;facets&quot;: { &quot;genresFacet&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;path&quot;: &quot;genres&quot; } } } } } ","keywords":"","version":"Next"},{"title":"üìò Why Atlas Search","type":0,"sectionRef":"#","url":"/search-lab/docs/full-text-search/why-atlas-search","content":"","keywords":"","version":"Next"},{"title":"How to use Atlas Search‚Äã","type":1,"pageTitle":"üìò Why Atlas Search","url":"/search-lab/docs/full-text-search/why-atlas-search#how-to-use-atlas-search","content":" To use Atlas Search, you need to create a search index. A search index is a data structure that contains the data you want to search and the rules for how to search it. You can create a search index using the Atlas UI or the Atlas API.  Once you have an index in place, you can use the MongoDB Query API to query your data. This will be done with the new $search operator in your aggregation pipelines. ","version":"Next","tagName":"h2"},{"title":"üìò How Atlas Search Works","type":0,"sectionRef":"#","url":"/search-lab/docs/full-text-search/how-search-works","content":"","keywords":"","version":"Next"},{"title":"Simple String Search‚Äã","type":1,"pageTitle":"üìò How Atlas Search Works","url":"/search-lab/docs/full-text-search/how-search-works#simple-string-search","content":" When you do a simple query in your database using a LIKE operator, or a regular expression, the database has to scan every document in the collection to find the matching documents. This is a slow process, and it gets slower as the number of documents in the collection increases.    ","version":"Next","tagName":"h2"},{"title":"Full Text Search‚Äã","type":1,"pageTitle":"üìò How Atlas Search Works","url":"/search-lab/docs/full-text-search/how-search-works#full-text-search","content":" Full-text search is meant to search large amounts of text. For example, a search engine will use a full-text search to look for keywords in all the web pages that it indexed. The key to this technique is indexing.  Indexing can be done in different ways, such as batch indexing or incremental indexing. The index then acts as an extensive glossary for any matching documents. Various techniques can then be used to extract the data. Apache Lucene, the open sourced search library, uses an inversed index to find the matching items. In the case of our menu search, each word links to the matching menu item.    This technique is much faster than string searches for large amounts of data.  ","version":"Next","tagName":"h2"},{"title":"Index Creation‚Äã","type":1,"pageTitle":"üìò How Atlas Search Works","url":"/search-lab/docs/full-text-search/how-search-works#index-creation","content":" In order to prepare your data to be indexed, your data will go through a process called tokenization. Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens. This is done through a series of analyzers. Analyzers are the building blocks of the search engine. They are responsible for producing tokens out of the text. The tokens are then stored in the index.  In our example, it will start by removing any diacritics (marks placed above or below letters, such as √©, √†, and √ß in French). Then, based on the used language, the algorithms will remove filler words and only keep the stem of the terms. This way, ‚Äúto eat,‚Äù ‚Äúeating,‚Äù and ‚Äúate‚Äù are all classified as the same ‚Äúeat‚Äù keyword. It then changes the casing to use only either uppercase or lowercase. The exact indexing process is determined by the analyzer that is used.    In the end, the index will look like a glossary of all the meaningful words in your data. Each word will be linked to the documents that contain it.   ","version":"Next","tagName":"h2"},{"title":"üìò Why Search","type":0,"sectionRef":"#","url":"/search-lab/docs/full-text-search/why-search","content":"üìò Why Search Search is a powerful tool that will create a better experience for your users. It will help them find the content they are looking for faster and more efficiently. It will also help them discover content they didn't know existed. More and more, users are expecting a Google-like search experience when they come to a website. They want to be able to type in a few words and get the content they are looking for. They don't want to have to navigate through a complex hierarchy of pages to find what they are looking for. If they can't find what they are looking for, they will leave your site and go somewhere else. This is especially true for mobile users who are often on the go and don't have time to navigate through a complex hierarchy of pages. Adding Search capabilities to your site There are many ways to add search capabilities to your site. One of the most popular ways is to use an open source search engine like Elasticsearch or Solr. These search engines are very powerful and can be used to index and search millions of documents. Behind the hood, both of these search engines use Lucene to index and search documents. Lucene is a Java library that provides a simple API for indexing and searching documents. To use these tools, you will need to implement a syncing mechanism between your data and the search engine. This can be a complex task and requires a lot of work.","keywords":"","version":"Next"},{"title":"üìò Complete Code example","type":0,"sectionRef":"#","url":"/search-lab/docs/hybrid-search/complete-example","content":"üìò Complete Code example Here's the complete example code to implement a hybrid search against the books collection. db.books.aggregate([ // Vector search on book synopsis embeddings { $vectorSearch: { index: &quot;books_synopsis_vector&quot;, path: &quot;embeddings&quot;, queryVector: [0.023, -0.345, 0.122, ...], // Example query vector numCandidates: 100, limit: 20 } }, // Add vector search score { $set: { vector_score: { $meta: &quot;vectorSearchScore&quot; } } }, // Combine with text search on title and author { $search: { index: &quot;books_text_index&quot;, compound: { should: [ { text: { query: &quot;mystery novel&quot;, path: &quot;title&quot;, score: { boost: { value: 3 } } } }, { text: { query: &quot;mystery novel&quot;, path: &quot;authors.name&quot;, score: { boost: { value: 2 } } } } ] } } }, // Add text search score { $set: { text_score: { $meta: &quot;searchScore&quot; } } }, // Combine scores { $addFields: { combined_score: { $add: [ { $multiply: [&quot;$vector_score&quot;, 0.6] }, { $multiply: [&quot;$text_score&quot;, 0.4] } ] } } }, // Sort and limit results { $sort: { combined_score: -1 } }, { $limit: 5 }, // Project final results { $project: { title: 1, authors: 1, synopsis: 1, combined_score: 1, vector_score: 1, text_score: 1 } } ]) ","keywords":"","version":"Next"},{"title":"üìò Introduction to Hybrid Search","type":0,"sectionRef":"#","url":"/search-lab/docs/hybrid-search/intro","content":"","keywords":"","version":"Next"},{"title":"What is Hybrid Search?‚Äã","type":1,"pageTitle":"üìò Introduction to Hybrid Search","url":"/search-lab/docs/hybrid-search/intro#what-is-hybrid-search","content":" Hybrid search is an advanced information retrieval technique that combines multiple search methodologies to provide more accurate and relevant results. In the context of our library management system, we'll be focusing on a hybrid approach that merges two powerful search paradigms:    Vector Search: Also known as semantic search, this method uses mathematical representations (vectors) of text to find similarities based on meaning rather than exact word matches.Full-Text Search: This traditional method looks for exact or close matches of words and phrases within text fields.  ","version":"Next","tagName":"h2"},{"title":"Why Use Hybrid Search?‚Äã","type":1,"pageTitle":"üìò Introduction to Hybrid Search","url":"/search-lab/docs/hybrid-search/intro#why-use-hybrid-search","content":" Hybrid search offers several advantages over using either vector or full-text search alone:  Improved Accuracy: By combining semantic understanding with keyword matching, hybrid search can capture both conceptual similarity and specific term relevance.Better Handling of Complex Queries: It can effectively process queries that might be ambiguous or require understanding of context.Enhanced User Experience: Users can find relevant results even when they don't know the exact terms to search for.Balancing Precision and Recall: Hybrid search helps in finding a good balance between returning highly specific results and a broader range of relevant items.  ","version":"Next","tagName":"h2"},{"title":"Hybrid Search in Our Library Management System‚Äã","type":1,"pageTitle":"üìò Introduction to Hybrid Search","url":"/search-lab/docs/hybrid-search/intro#hybrid-search-in-our-library-management-system","content":" In our library application, hybrid search will allow us to:  Find books based on the semantic similarity of their synopses to a given query.Match books by title, author, or other metadata fields.Combine these approaches to rank results in a way that considers both content similarity and metadata relevance.  For example, a search for &quot;space exploration&quot; could return:  Books directly about space exploration (matched by title or synopsis)Science fiction novels set in space (matched semantically)Biographies of astronauts (matched by related concepts)  ","version":"Next","tagName":"h2"},{"title":"How It Works‚Äã","type":1,"pageTitle":"üìò Introduction to Hybrid Search","url":"/search-lab/docs/hybrid-search/intro#how-it-works","content":" Our hybrid search implementation will:  Use vector search on book synopsis embeddings to find semantically similar content.Employ full-text search on titles, author names, and other relevant fields.Combine the scores from both methods to create a final relevance ranking.  This approach ensures that our library catalog search is both powerful and intuitive, helping users discover books that best match their interests, even if they're not sure of the exact titles or authors they're looking for. ","version":"Next","tagName":"h2"},{"title":"üìò Building the Hybrid Search Pipeline","type":0,"sectionRef":"#","url":"/search-lab/docs/hybrid-search/implementing-hybrid-search","content":"","keywords":"","version":"Next"},{"title":"3.1 Vector Search Stage‚Äã","type":1,"pageTitle":"üìò Building the Hybrid Search Pipeline","url":"/search-lab/docs/hybrid-search/implementing-hybrid-search#31-vector-search-stage","content":" The first stage of our pipeline will be the vector search. This allows us to find books with synopses that are semantically similar to our query.   { $vectorSearch: { index: &quot;books_synopsis_vector&quot;, path: &quot;embeddings&quot;, queryVector: [0.1, -0.2, 0.3, ...], // Your query vector here numCandidates: 100, limit: 20 } }   Let's break this down:  index: The name of your vector index.path: The field containing your embeddings.queryVector: The vector representation of your search query.numCandidates: The number of initial candidates to consider.limit: The maximum number of results to return from this stage.  ","version":"Next","tagName":"h2"},{"title":"3.2 Text Search Stage‚Äã","type":1,"pageTitle":"üìò Building the Hybrid Search Pipeline","url":"/search-lab/docs/hybrid-search/implementing-hybrid-search#32-text-search-stage","content":" Next, we'll add a text search stage to find books based on title and author matches.  { $search: { index: &quot;books_text_index&quot;, compound: { should: [ { text: { query: &quot;your search query&quot;, path: &quot;title&quot;, score: { boost: { value: 3 } } } }, { text: { query: &quot;your search query&quot;, path: &quot;authors.name&quot;, score: { boost: { value: 2 } } } } ] } } }   ","version":"Next","tagName":"h2"},{"title":"Key points:‚Äã","type":1,"pageTitle":"üìò Building the Hybrid Search Pipeline","url":"/search-lab/docs/hybrid-search/implementing-hybrid-search#key-points","content":" We're using a compound query with &quot;should&quot; clauses.We search in both title and authors.name fields.The boost values (3 for title, 2 for author) give more weight to title matches. ","version":"Next","tagName":"h2"},{"title":"üìò Introduction","type":0,"sectionRef":"#","url":"/search-lab/docs/intro","content":"","keywords":"","version":"Next"},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"üìò Introduction","url":"/search-lab/docs/intro#prerequisites","content":" If you are following this workshop in the context of a Developer Day, you are already good to go. If not, go through the following workshop first to create you Atlas cluster, import data, and view the sample application.  Intro Lab  In this lab, we will also assume some basic knowledge of aggregation pipelines. If you are not familiar with them, you can go through the advanced querying workshop first.  Advanced Querying With Aggregation Pipelines  ","version":"Next","tagName":"h2"},{"title":"üìòüëêü¶∏ Navigation bar icons‚Äã","type":1,"pageTitle":"üìò Introduction","url":"/search-lab/docs/intro#-navigation-bar-icons","content":" These labs are meant to be presented by one of our amazing instructors, but you can also do them on your own.  In the navigation bar, you will notice some icons. Here is their meaning:  Icon\tMeaningüìò\tLecture material - If you're following along in an instructor lead session, they probably have covered this already. üëê\tHands-on content - Get ready to get some work done. You should follow these steps. ü¶∏\tAdvanced content - This content isn't covered during the lab, but if you're interested in learning more, you can check it out. ","version":"Next","tagName":"h2"},{"title":"üìò The equals operator","type":0,"sectionRef":"#","url":"/search-lab/docs/search-operators/equals","content":"üìò The equals operator The equals operator is used to search for a specific value in a field. It is similar to the match operator, but the operation will be performed by Lucene, and can be combined with other operators to refine results. The equals operator has the following syntax. You can find more explanation about it in the documentation. { $search: { &quot;index&quot;: &lt;index name&gt;, // optional, defaults to &quot;default&quot; &quot;equals&quot;: { &quot;path&quot;: &quot;&lt;field-to-search&gt;&quot;, &quot;value&quot;: &lt;boolean-value&gt;|&lt;objectId&gt;|&lt;number&gt;|&lt;date&gt;, &quot;score&quot;: &lt;score-options&gt; } } } This could be useful if you wanted to query only for books that were published in a specific year.","keywords":"","version":"Next"},{"title":"ü¶∏ Advanced Exercises","type":0,"sectionRef":"#","url":"/search-lab/docs/search-operators/advanced-exercises","content":"","keywords":"","version":"Next"},{"title":"Promote shorter books‚Äã","type":1,"pageTitle":"ü¶∏ Advanced Exercises","url":"/search-lab/docs/search-operators/advanced-exercises#promote-shorter-books","content":" You've noticed that readers tend to prefer shorter books. Without using a large score boost, try to promote shorter books in the search results.  tip Even without a score adjuster, the should operator will still boost the score of the documents that match it.  Answer public async searchBooks(query: string): Promise&lt;Book[]&gt; { const aggregationPipeline = [ { $search: { &quot;index&quot;: &quot;fulltextsearch&quot;, &quot;compound&quot;: { &quot;must&quot;: [ { &quot;text&quot;: { query, &quot;path&quot;: [&quot;title&quot;, &quot;authors.name&quot;, &quot;genres&quot;], fuzzy: { maxEdits: 2 } } } ], &quot;should&quot;: [ { &quot;equals&quot;: { &quot;value&quot;: true, &quot;path&quot;: &quot;bookOfTheMonth&quot;, &quot;score&quot;: { &quot;boost&quot;: { value: 10 } } } }, { &quot;range&quot;: { path: &quot;pages&quot;, lt: 80 } } ] } } } ]; const books = await collections?.books?.aggregate(aggregationPipeline).toArray() as Book[]; return books; }   Choose your favourite method to build aggregation pipelines and try to solve the following exercises.  ","version":"Next","tagName":"h2"},{"title":"Most available book of the month books‚Äã","type":1,"pageTitle":"ü¶∏ Advanced Exercises","url":"/search-lab/docs/search-operators/advanced-exercises#most-available-book-of-the-month-books","content":" Build a search aggregation stage that will promote the books of the month, and will return the books with the most copies available first.  ","version":"Next","tagName":"h2"},{"title":"Most prolific pooh author‚Äã","type":1,"pageTitle":"ü¶∏ Advanced Exercises","url":"/search-lab/docs/search-operators/advanced-exercises#most-prolific-pooh-author","content":" Find out which author has written the most books that contains the word &quot;pooh&quot; (we're talking about the bear here) in the title or in the synopsis.  ","version":"Next","tagName":"h2"},{"title":"Assume typos, but still find the matches‚Äã","type":1,"pageTitle":"ü¶∏ Advanced Exercises","url":"/search-lab/docs/search-operators/advanced-exercises#assume-typos-but-still-find-the-matches","content":" Find books that match a user query, but also return books that match with a typo. These books should be returned after the books that match the query. ","version":"Next","tagName":"h2"},{"title":"ü¶∏‚Äç‚ôÇÔ∏è Hybrid Search Exercise","type":0,"sectionRef":"#","url":"/search-lab/docs/hybrid-search/exercise","content":"","keywords":"","version":"Next"},{"title":"Creating a Basic Hybrid Search Pipeline‚Äã","type":1,"pageTitle":"ü¶∏‚Äç‚ôÇÔ∏è Hybrid Search Exercise","url":"/search-lab/docs/hybrid-search/exercise#creating-a-basic-hybrid-search-pipeline","content":" Let's start by creating a basic hybrid search pipeline that combines vector search on book synopses with text search on titles and author names:  [ { $vectorSearch: { queryVector: vector, // Assume this is already defined path: &quot;embeddings&quot;, numCandidates: 100, index: &quot;books_synopsis_vector&quot;, limit: 20 } }, { $search: { index: &quot;books_text_index&quot;, compound: { should: [ { text: { query: searchQuery, // Assume this is already defined path: &quot;title&quot;, score: { boost: { value: 3 } } } }, { text: { query: searchQuery, path: &quot;authors.name&quot;, score: { boost: { value: 2 } } } } ] } } }, { $addFields: { vector_score: { $meta: &quot;vectorSearchScore&quot; }, text_score: { $meta: &quot;searchScore&quot; } } }, { $addFields: { combined_score: { $add: [ { $multiply: [&quot;$vector_score&quot;, 0.5] }, { $multiply: [&quot;$text_score&quot;, 0.5] } ] } } }, { $sort: { combined_score: -1 } }, { $limit: 10 } ]   Add this aggregation pipeline to your code in server/src/controllers/books.ts inside the searchBooks method.  ","version":"Next","tagName":"h2"},{"title":"Experimenting with Score Weighting‚Äã","type":1,"pageTitle":"ü¶∏‚Äç‚ôÇÔ∏è Hybrid Search Exercise","url":"/search-lab/docs/hybrid-search/exercise#experimenting-with-score-weighting","content":" Now that we have a basic hybrid search implemented, let's experiment with different weightings for the vector and text scores.  Try adjusting the weights in the combined_score calculation. For example:  combined_score: { $add: [ { $multiply: [&quot;$vector_score&quot;, 0.7] }, { $multiply: [&quot;$text_score&quot;, 0.3] } ] }   This gives more weight to the vector search results.  Test the search with different queries and observe how the results change with different weightings.Experiment with the boost values in the text search stage. Try increasing the boost for the title or author name and see how it affects the results.  ","version":"Next","tagName":"h2"},{"title":"Adding Pre-filtering‚Äã","type":1,"pageTitle":"ü¶∏‚Äç‚ôÇÔ∏è Hybrid Search Exercise","url":"/search-lab/docs/hybrid-search/exercise#adding-pre-filtering","content":" To further refine our hybrid search, let's add pre-filtering capabilities. We'll filter books by their publication year before performing the vector search.  Modify your vector search stage to include a filter:   { $vectorSearch: { queryVector: vector, path: &quot;embeddings&quot;, numCandidates: 100, index: &quot;books_synopsis_vector&quot;, limit: 20, filter: { year: { $gte: 2000 } } // Only books published from 2000 onwards } }   Remember to update your vector index to support filtering on the year field, similar to what you did in the pre-filtering exercise:  { &quot;fields&quot;: [ { &quot;type&quot;: &quot;vector&quot;, &quot;path&quot;: &quot;embeddings&quot;, &quot;numDimensions&quot;: 1536, &quot;similarity&quot;: &quot;cosine&quot; }, { &quot;type&quot;: &quot;filter&quot;, &quot;path&quot;: &quot;year&quot; } ] }   ","version":"Next","tagName":"h2"},{"title":"Exercise Tasks‚Äã","type":1,"pageTitle":"ü¶∏‚Äç‚ôÇÔ∏è Hybrid Search Exercise","url":"/search-lab/docs/hybrid-search/exercise#exercise-tasks","content":" Implement the basic hybrid search pipeline in your application.Experiment with at least three different weight combinations for vector and text scores. Document how the results change.Add pre-filtering to your hybrid search to only include books published in the last 20 years.Create a function that allows users to specify the importance of title matches vs. content similarity, and adjust the weights accordingly.Test your hybrid search with various queries and compare the results to those from pure vector search and pure text search.  Completing this exercise, you'll gain hands-on experience in implementing and fine-tuning a hybrid search solution, combining the strengths of both vector and text search capabilities in MongoDB Atlas. ","version":"Next","tagName":"h2"},{"title":"üìò Intro to Search operators","type":0,"sectionRef":"#","url":"/search-lab/docs/search-operators/intro","content":"üìò Intro to Search operators In the previous exercise, you used a $search stage that looked like this one. { $search: { index: 'fulltextsearch', text: { query, path: ['title', 'authors.name', 'categories'] } } } We've talked about the $search stage, and what is the index property, but so far, we've only used the text operator in it's most simple form. However, as you want to refine the results of your search, you'll need to use more advanced operators, and even combine some of those operators to get the results you want. In this exercise, we'll explore some of the most common operators, and how to use them.","keywords":"","version":"Next"},{"title":"üìò The range operator","type":0,"sectionRef":"#","url":"/search-lab/docs/search-operators/range","content":"üìò The range operator The range operator is similar to equals, but works on ranges of values. The range operator has the following syntax. You can find more explanation about it in the documentation. { &quot;$search&quot;: { &quot;index&quot;: &lt;index name&gt;, // optional, defaults to &quot;default&quot; &quot;range&quot;: { &quot;path&quot;: &quot;&lt;field-to-search&gt;&quot;, &quot;gt | gte&quot;: &lt;value-to-search&gt;, &quot;lt | lte&quot;: &lt;value-to-search&gt;, &quot;score&quot;: &lt;score-options&gt; } } } This could be useful if you wanted to query only for books that have a range of pages. Say you only want books between 100 and 300 pages, you could use the range operator to refine your results.","keywords":"","version":"Next"},{"title":"üìò Mixing and matching","type":0,"sectionRef":"#","url":"/search-lab/docs/search-operators/mix-and-match","content":"","keywords":"","version":"Next"},{"title":"Compound with scoring‚Äã","type":1,"pageTitle":"üìò Mixing and matching","url":"/search-lab/docs/search-operators/mix-and-match#compound-with-scoring","content":" You can also use the compound operator with the score option to control how the scores of the subqueries are combined. Say the library has a monthly theme, and they want to promote the books that have a specific word in the synopsis. This month, the theme is &quot;bears&quot;. Try to write a search query that will return the books a user queried for, but those who contain the word &quot;bear&quot; in it will have a higher score and come at the top. Try to have a book about Winnie the Pooh come first when a user searches for &quot;honey&quot;.  Click here to see the answer { $search: { &quot;index&quot;: &quot;fulltextsearch&quot;, &quot;compound&quot;: { &quot;must&quot;: [{ &quot;text&quot;: { &quot;query&quot;: &quot;honey&quot;, &quot;path&quot;: [&quot;title&quot;, &quot;author.name&quot;, &quot;synopsis&quot;] } }], &quot;should&quot;: [{ &quot;text&quot;: { &quot;query&quot;: &quot;bear&quot;, &quot;path&quot;: &quot;synopsis&quot;, &quot;score&quot;: { &quot;boost&quot;: { value: 10 } } } }] } } }  ","version":"Next","tagName":"h2"},{"title":"üìò The phrase operator","type":0,"sectionRef":"#","url":"/search-lab/docs/search-operators/phrase","content":"","keywords":"","version":"Next"},{"title":"The slop property‚Äã","type":1,"pageTitle":"üìò The phrase operator","url":"/search-lab/docs/search-operators/phrase#the-slop-property","content":" The slop property is used to specify how far apart the words in the phrase can be. The default value is 0, which means that the words must be in the exact order specified in the query. If you set the value to 1, then the words can be one word apart, and so on. ","version":"Next","tagName":"h2"},{"title":"üìò The text operator","type":0,"sectionRef":"#","url":"/search-lab/docs/search-operators/text","content":"","keywords":"","version":"Next"},{"title":"The fuzzy property‚Äã","type":1,"pageTitle":"üìò The text operator","url":"/search-lab/docs/search-operators/text#the-fuzzy-property","content":" Fuzzy search is a technique that allows you to search for terms that are similar to the one you are looking for. It is useful when your users might make typos or spelling mistakes in their search queries.  There are many options you can use with the fuzzy property, and you can find out more in the docs. For now, we'll focus on the maxEdits option. MaxEdits will specify the number of one character edits that can be made to find a match.  For example, if you search for Alice, you should find the book Alice in Wonderland. However, if you make one, or multiple typos, such as Alyse, Alise, or Allice, you will not find the book. This is where the maxEdits option comes in handy. With a maxEdits of 1, you will find the book even if you make one typo.  ","version":"Next","tagName":"h2"},{"title":"The synonyms property‚Äã","type":1,"pageTitle":"üìò The text operator","url":"/search-lab/docs/search-operators/text#the-synonyms-property","content":" The synonyms property allows you to specify a mapping of synonyms that will be used in your search. For example, if you search for car, you might want to find documents that contain the word automobile as well. You can specify a mapping of synonyms in your index configuration, and then use the synonyms property in your search query to use that mapping. ","version":"Next","tagName":"h2"},{"title":"üëê Exercises","type":0,"sectionRef":"#","url":"/search-lab/docs/search-operators/exercises","content":"","keywords":"","version":"Next"},{"title":"Add fuzzy search‚Äã","type":1,"pageTitle":"üëê Exercises","url":"/search-lab/docs/search-operators/exercises#add-fuzzy-search","content":" Try adding fuzzy search to your application so it can find the correct books, even if the user makes a typo.  tip Remember the fuzzy parameter of the text operator? This is where you'd use it.  Answer public async searchBooks(query: string): Promise&lt;Book[]&gt; { const aggregationPipeline = [ { $search: { index: 'fulltextsearch', text: { query, path: ['title', 'authors.name', 'genres'], fuzzy: { maxEdits: 2 } } } } ]; const books = await collections?.books?.aggregate(aggregationPipeline).toArray() as Book[]; return books; }   ","version":"Next","tagName":"h2"},{"title":"Promote the books of the month‚Äã","type":1,"pageTitle":"üëê Exercises","url":"/search-lab/docs/search-operators/exercises#promote-the-books-of-the-month","content":" The marketing team has decided that they want to promote the books of the month. They want to show these books first in the search results. How would you do this?  tip You'll need to use the compound operator with multiple operators here. There is a bookOfTheMonth boolean field on some of the books that you could use.  Answer public async searchBooks(query: string): Promise&lt;Book[]&gt; { const aggregationPipeline = [ { $search: { &quot;index&quot;: &quot;fulltextsearch&quot;, &quot;compound&quot;: { &quot;must&quot;: [ { &quot;text&quot;: { query, &quot;path&quot;: [&quot;title&quot;, &quot;authors.name&quot;, &quot;genres&quot;], fuzzy: { maxEdits: 2 } } } ], &quot;should&quot;: [ { &quot;equals&quot;: { &quot;value&quot;: true, &quot;path&quot;: &quot;bookOfTheMonth&quot;, &quot;score&quot;: { &quot;boost&quot;: { value: 10 } } } } ] } } } ]; const books = await collections?.books?.aggregate(aggregationPipeline).toArray() as Book[]; return books; }  ","version":"Next","tagName":"h2"},{"title":"üìò Score modifiers","type":0,"sectionRef":"#","url":"/search-lab/docs/search-operators/score-modifier","content":"","keywords":"","version":"Next"},{"title":"Boost‚Äã","type":1,"pageTitle":"üìò Score modifiers","url":"/search-lab/docs/search-operators/score-modifier#boost","content":" Using boost, you can multiply the score by a certain value. This can be useful to promote certain results that would be sponsored for example. The higher sponsor tiers would get higher modifiers.  In our dataset, we could decide to boost results that are of the fiction genre.  { $search: { index: 'fulltextsearch', text: { query: 'fiction', path: 'genres', score: { boost: { value: 2 }} } } }   ","version":"Next","tagName":"h2"},{"title":"Constant‚Äã","type":1,"pageTitle":"üìò Score modifiers","url":"/search-lab/docs/search-operators/score-modifier#constant","content":" We can also set the score to a constant value. This can be useful to demote results that are not relevant enough. In our use case, we could set a constant value of 0 for books that are not currently available.  { $search: { index: 'fulltextsearch', equals: { path: 'available', value: 0, score: { constant: { value: 0 } } } } }  ","version":"Next","tagName":"h2"},{"title":"üìò Scoring","type":0,"sectionRef":"#","url":"/search-lab/docs/search-operators/scoring","content":"üìò Scoring You might have noticed that most of the operators have a score property, which we haven't really talked about so far. Before we jump into scoring, it is important to understand what the document score is. Every document returned by an Atlas Search query is assigned a score based on relevance, and the documents included in a result set are returned in order from highest score to lowest. Some factors that can influence the score include: The position of the search term in the document,The frequency of occurrence of the search term in the document,The type of operator the query uses,The type of analyzer the query uses. To see the scores, you will need to project a new metadata field, using $meta. To see the document scores in a simple phrase search for Alice in Wonderland, try the followin aggregation pipeline. You can run this in the aggregation builders from the Atlas UI or in Compass. [ { $search: { index: &quot;fulltextsearch&quot;, phrase: { query: &quot;Alice in Wonderland&quot;, path: &quot;title&quot;, }, }, }, { $project: { title: 1, authors: 1, pages: 1, year: 1, score: { $meta: &quot;searchScore&quot;, }, }, }, ] Your first result should look be { &quot;_id&quot;: &quot;0831702877&quot;, &quot;title&quot;: &quot;Alice in Wonderland&quot;, &quot;authors&quot;: [ { &quot;_id&quot;: &quot;64cc2db4830ba29148da7bbc&quot;, &quot;name&quot;: &quot;Walt Disney Productions&quot; }, { &quot;_id&quot;: &quot;64cc2db4830ba29148da8db5&quot;, &quot;name&quot;: &quot;Mouse Works&quot; } ], &quot;pages&quot;: 95, &quot;year&quot;: 1997, &quot;score&quot;: 10.441896438598633 } It got scored much higher than the next one because it's an exact match.","keywords":"","version":"Next"},{"title":"üëê Getting Started","type":0,"sectionRef":"#","url":"/search-lab/docs/search/intro","content":"üëê Getting Started In this section, you will see how easy it can be to get started with MongoDB Atlas Search. caution At this point, you should already have the sample application running in a Codespace, along with a free cluster with the library data loaded. If you do not have this, please go through the Intro Lab first.","keywords":"","version":"Next"},{"title":"üëê Create an Atlas Search index","type":0,"sectionRef":"#","url":"/search-lab/docs/search/search-index","content":"","keywords":"","version":"Next"},{"title":"Step-by-step guide to creating your first Atlas Search index‚Äã","type":1,"pageTitle":"üëê Create an Atlas Search index","url":"/search-lab/docs/search/search-index#step-by-step-guide-to-creating-your-first-atlas-search-index","content":" Open the Database Deployments page in MongoDB Atlas and select Create Index in the lower right corner.    Click the Create Search Index button.    The first step of building the search index is selecting the configuration method. You can choose from two options ‚Äî using the Visual Editor or writing the configuration yourself with the JSON Editor. Let's stick to the default Visual Editor. To proceed, click Next.    Next, you need to select a name and data source for your index. Change the name to fulltextsearch and select the database library and the collection books.    The final step allows you to review the index configuration and refine it if needed. You may also see the JSON that was generated from your configuration by clicking View JSON.  { &quot;mappings&quot;: { &quot;dynamic&quot;: true } }   info The index is using dynamic field mappings. We didn't configure any explicit (static) mappings between the fields in the documents and the search index. That's why Atlas created dynamic mappings that match the data in the documents to some common field types such as double, string, array, int, and double. Dynamic mappings are useful when you're just getting started with Atlas Search or if your schema changes regularly. However, they take up more space compared to static mappings.  You don't need to refine this index. Go ahead and click Create Search Index.    You'll be redirected to a page showing all of the search indexes used in your Atlas project. Creating the index should take up to a minute.  info When your search index reaches status Active, you'll be able to see more information about the index. For example, the number of indexed documents, the field mappings and the index size. Notice also that you're using 1 out of the 3 free search indexes that come with your free M0 database.    Once you see your new index showing as Active in the list, you can move to the next step. ","version":"Next","tagName":"h2"},{"title":"üëê Testing the Search index","type":0,"sectionRef":"#","url":"/search-lab/docs/search/test-search","content":"","keywords":"","version":"Next"},{"title":"Search Tester‚Äã","type":1,"pageTitle":"üëê Testing the Search index","url":"/search-lab/docs/search/test-search#search-tester","content":" By using the Query button above, you will open the Search Tester. In the field, enter the word:  cooking   and look at the results you get.    You can see that you get a list of books back including:  Pennsylvania Dutch Cooking: A Mennonite Community CookbookSemi-Homemade Cooking: Quick, Marvelous Meals and Nothing is Made from‚Ä¶Cookin' Cajun Cooking School Cookbook - Creole and Cajun Cuisine from ‚Ä¶  All those results match the word cooking. Notice how capitalization doesn't matter. Neither does the placement of the word in the title. The search index is smart enough to find the word cooking in all those titles.  Already, you are providing a better experience to your developers.  ","version":"Next","tagName":"h2"},{"title":"Searching only the title‚Äã","type":1,"pageTitle":"üëê Testing the Search index","url":"/search-lab/docs/search/test-search#searching-only-the-title","content":" This is great. However, this search query is searching the entire document. There might be some cases where that works for you, but in most cases, you'll likely want to limit this search only to the fields that matter. In this case, let's change it to only search the title.  In the Search Tester, click on the Edit $search query button, next to the green Search button.  This will open up the query editor.    Notice that the path property currently shows *. This means that it will search the entire document. Try to change the search to only search the title field.  tip You can find more information about how to construct a query path in the Search documentation.  Click here to see the answer [ { $search: { index: &quot;fulltextsearch&quot;, text: { query: &quot;cooking&quot;, path: [&quot;title&quot;] } } } ]   You will see the same results, but ordered differently. This has to do with relevancy, but we'll cover that in a later section. ","version":"Next","tagName":"h2"},{"title":"üéØ Summary","type":0,"sectionRef":"#","url":"/search-lab/docs/summary","content":"üéØ Summary Congratulations! Following this tutorial, you have successfully: Learned what full-text search and Atlas Search are.Added full-text search to your application.Added semantic search to your application. Visit the MongoDB Developer Center for more useful information and tutorials.","keywords":"","version":"Next"},{"title":"üëê Add semantic search to your application","type":0,"sectionRef":"#","url":"/search-lab/docs/vector-search/add-to-app","content":"","keywords":"","version":"Next"},{"title":"Configuring the environment variables‚Äã","type":1,"pageTitle":"üëê Add semantic search to your application","url":"/search-lab/docs/vector-search/add-to-app#configuring-the-environment-variables","content":" In your server/.env file, you'll find a few variables that you can use to configure the application. Add a couple more at the end of the file to configure the embeddings source and the API key.  EMBEDDINGS_SOURCE=serverlessEndpoint EMBEDDING_KEY=&lt;API Key&gt;   tip Your instructor will provide you with an API key that you can use for the event you're attending. Set it in the EMBEDDING_KEY variable.  ","version":"Next","tagName":"h2"},{"title":"Configuring the vector search query‚Äã","type":1,"pageTitle":"üëê Add semantic search to your application","url":"/search-lab/docs/vector-search/add-to-app#configuring-the-vector-search-query","content":" Open up the code from the server file server/src/controllers/books.ts once more, and edit the searchBooks method to query your data for semantic search.  tip Use the getEmbeddings function to convert the query into a vector.  Answer public async searchBooks(query: string): Promise&lt;Book[]&gt; { const vector = await getEmbeddings(query); const aggregationPipeline = [ { $vectorSearch: { queryVector: vector, path: 'embeddings', numCandidates: 100, index: 'vectorsearch', limit: 100, } } ]; const books = await collections?.books?.aggregate(aggregationPipeline).toArray() as Book[]; return books; }   ","version":"Next","tagName":"h2"},{"title":"Testing the semantic search‚Äã","type":1,"pageTitle":"üëê Add semantic search to your application","url":"/search-lab/docs/vector-search/add-to-app#testing-the-semantic-search","content":" To test the semantic search in the app, try to search for some books but use different words that have a similar meaning or are related to the book's cover.  You can use the following queries:  Canines doing stuffFluffy animalsEuropean history ","version":"Next","tagName":"h2"},{"title":"Using OpenAI","type":0,"sectionRef":"#","url":"/search-lab/docs/vector-search/create-vectors/openai","content":"","keywords":"","version":"Next"},{"title":"üëê Create Vector Search indexes","type":0,"sectionRef":"#","url":"/search-lab/docs/vector-search/create-index","content":"","keywords":"","version":"Next"},{"title":"Step-by-step guide to creating your Vector Search index‚Äã","type":1,"pageTitle":"üëê Create Vector Search indexes","url":"/search-lab/docs/vector-search/create-index#step-by-step-guide-to-creating-your-vector-search-index","content":" Open the Database Deployments page in MongoDB Atlas and select Create Index in the lower right corner.    Click the Create Search Index button.    This time, you will use the JSON Editor to create your index    Select your database and the books collection, change the index name to vectorsearch, and add the following code in the JSON editor:  Serverless EndpointOpenAIGoogle Cloud Vertex AI { &quot;fields&quot;:[ { &quot;type&quot;: &quot;vector&quot;, &quot;path&quot;: &quot;embeddings&quot;, &quot;numDimensions&quot;: 1408, &quot;similarity&quot;: &quot;cosine&quot; } ] }   The final step allows you to review the index configuration and refine it if needed. Go ahead and click Create Search Index. ","version":"Next","tagName":"h2"},{"title":"Create an OpenAI account and get an API key‚Äã","type":1,"pageTitle":"Using OpenAI","url":"/search-lab/docs/vector-search/create-vectors/openai#create-an-openai-account-and-get-an-api-key","content":" To create an account, go to https://openai.com/ and click on the &quot;Log In&quot; button in the upper right corner. This will redirect you to the login page, where you'll have the option to sign up for their services.    Follow the instructions on the screen, and verify your email address.  Once you have an account, you can go to the API keys page to get an API key.  From there, click on the Create new secret key button.    You'll be prompted to give your key a name. You can call it &quot;MongoDB Vector Search Demo&quot;. Then click on the Create secret key button.  You will then be presented with your API key. Copy it and save it somewhere safe.    caution Make sure you copy this key somewhere as you'll need it later on, and you won't be able to see it again.  Now that you have an API key, you can use it to create embeddings for your documents.  ","version":"Next","tagName":"h2"},{"title":"Create embeddings for documents‚Äã","type":1,"pageTitle":"Using OpenAI","url":"/search-lab/docs/vector-search/create-vectors/openai#create-embeddings-for-documents","content":" To create embeddings for your documents by sending curl commands to the OpenAI API, you can use the following command.  OPENAI_API_KEY=&lt;YOUR_API_KEY&gt; curl https://api.openai.com/v1/embeddings \\ -H &quot;Authorization: Bearer $OPENAI_API_KEY&quot; \\ -H &quot;Content-Type: application/json&quot; \\ -d '{ &quot;input&quot;: &quot;The food was delicious and the waiter...&quot;, &quot;model&quot;: &quot;text-embedding-ada-002&quot; }'   You can find more information about the API in the OpenAI documentation.  ","version":"Next","tagName":"h2"},{"title":"Create embeddings for the books‚Äã","type":1,"pageTitle":"Using OpenAI","url":"/search-lab/docs/vector-search/create-vectors/openai#create-embeddings-for-the-books","content":" To create the embeddings for the books in your collection, you should run this curl command, or use the Node.js library, for each book. This process is somewhat time consuming, so we've already created them for you.  You can find the 1586 dimensions vector in embeddings field of the books.  Because we already have the vectors for the books, we can use them with Vector Search.  ","version":"Next","tagName":"h2"},{"title":"Querying with vectors‚Äã","type":1,"pageTitle":"Using OpenAI","url":"/search-lab/docs/vector-search/create-vectors/openai#querying-with-vectors","content":" To query the data, Vector Search will need to calculate the distance between the query vector and the vectors of the documents in the collection.  To do so, you will need to vectorize your query. You can use the same function to vectorize your query as well.  In the library application, we've created a function that will vectorize your query for you. You can find it in the server/src/embeddings/openai.ts file.  import OpenAI from 'openai'; const { EMBEDDING_KEY } = process.env; let openai; const getTermEmbeddings = async (text) =&gt; { if (!openai) { openai = new OpenAI({apiKey: EMBEDDING_KEY}); } const embeddings = await openai.embeddings.create({ model: 'text-embedding-ada-002', input: text, }); return embeddings?.data[0]?.embedding; }; export default getTermEmbeddings;   ","version":"Next","tagName":"h2"},{"title":"Configuring the application‚Äã","type":1,"pageTitle":"Using OpenAI","url":"/search-lab/docs/vector-search/create-vectors/openai#configuring-the-application","content":" In your server/.env file, you'll find a few variables that you can use to configure the application.  The first one is EMBEDDINGS_SOURCE. It tells the application where to get the embeddings from. You can set it to openai.  Now that you have an OpenAI API key, you can then set the EMBEDDING_KEY variable to your API key.  EMBEDDINGS_SOURCE=openai EMBEDDING_KEY=sk-...  ","version":"Next","tagName":"h2"},{"title":"Using Amazon SageMaker","type":0,"sectionRef":"#","url":"/search-lab/docs/vector-search/create-vectors/ws-sagemaker","content":"","keywords":"","version":"Next"},{"title":"Getting Started With Amazon SageMaker‚Äã","type":1,"pageTitle":"Using Amazon SageMaker","url":"/search-lab/docs/vector-search/create-vectors/ws-sagemaker#getting-started-with-amazon-sagemaker","content":" Just follow the steps in part 2 of this tutorial, adapting it to the library information we've been using.  Create Your Model Endpoint With Amazon SageMaker, AWS Lambda, and AWS API Gateway ","version":"Next","tagName":"h2"},{"title":"üìò Implementing Vector Search","type":0,"sectionRef":"#","url":"/search-lab/docs/vector-search/implementing-vector-search","content":"","keywords":"","version":"Next"},{"title":"Vectorize your dataset‚Äã","type":1,"pageTitle":"üìò Implementing Vector Search","url":"/search-lab/docs/vector-search/implementing-vector-search#vectorize-your-dataset","content":" First, you will need to create this mathematical representation of your data. This is called vectorization. In the Create Vectors section, you will learn how to create vectors with some of the major encoders.  In the context of this workshop, however, we already did the work for you. You will be able to import the data with the embeddings using the import tool you've used previously.  ","version":"Next","tagName":"h2"},{"title":"Vectorize your query‚Äã","type":1,"pageTitle":"üìò Implementing Vector Search","url":"/search-lab/docs/vector-search/implementing-vector-search#vectorize-your-query","content":" The second component is to vectorize your query. This is the same process as vectorizing your dataset, but instead of vectorizing a large dataset, you are vectorizing a single query.  It is important to use the same encoder for both your dataset and your query. This is because the encoder learns a specific way to represent the data. If you use a different encoder, the vectors will be different and the search will not work.  For this workshop, you will be using one of the provided functions to vectorize your query. Again, the full code to do this on your own is in the Create Vectors section. ","version":"Next","tagName":"h2"},{"title":"üëê Import Vectorized Data","type":0,"sectionRef":"#","url":"/search-lab/docs/vector-search/import-data","content":"","keywords":"","version":"Next"},{"title":"Import the embeddings‚Äã","type":1,"pageTitle":"üëê Import Vectorized Data","url":"/search-lab/docs/vector-search/import-data#import-the-embeddings","content":" To import the dataset with the pre-created embeddings, you can use the import tool that you used earlier.  Enter your connection string in the text box at the top, and pick your provider from the dropdown at the bottom of the screen.    tip If unsure, or don't want to create your own embeddings, you can use the Serverless Endpoint provider.  Once you picked your provider, click on the Add Embeddings button.  warning This process will drop your existing books collection to replace it with a new one. Any existing indexes or changes to your collection will be lost.  Once the import is complete, you will see a confirmation message. ","version":"Next","tagName":"h2"},{"title":"ü¶∏‚Äç‚ôÇÔ∏è Pre-Filtering data","type":0,"sectionRef":"#","url":"/search-lab/docs/vector-search/filtering","content":"","keywords":"","version":"Next"},{"title":"Pre-filtering using number fields‚Äã","type":1,"pageTitle":"ü¶∏‚Äç‚ôÇÔ∏è Pre-Filtering data","url":"/search-lab/docs/vector-search/filtering#pre-filtering-using-number-fields","content":" If we want to pre-filter all books that are from 2001 we can try this (but it won't work right now):  [ {$vectorSearch: { queryVector: vector, path: &quot;embeddings&quot;, numCandidates: 100, index: &quot;vectorsearch&quot;, limit: 100, filter: {year: {$eq: 2001}} } } ]   ","version":"Next","tagName":"h2"},{"title":"Pre-filtering using string fields‚Äã","type":1,"pageTitle":"ü¶∏‚Äç‚ôÇÔ∏è Pre-Filtering data","url":"/search-lab/docs/vector-search/filtering#pre-filtering-using-string-fields","content":" We can try this and it won't work as well:  [ {$vectorSearch: { queryVector: vector, path: &quot;embeddings&quot;, numCandidates: 100, index: &quot;vectorsearch&quot;, limit: 100, filter: {language: {$eq: &quot;es&quot;}} } } ]   The problem lies on the vectorsearch index, not in this code. For String fields to be pre-filtered we need to add a mapping to those fields in our Search Index definition. To do that, go to MongoDB Atlas, go to your collections and open again the Search Indexes tab, as you did while creating the indexes  In this case, we already have our index and we're going to edit it in the JSON editor. Just change the index adding a mapping for the year field and language field. The index should look like:  { &quot;fields&quot;: [ { &quot;type&quot;: &quot;vector&quot;, &quot;path&quot;: &quot;embeddings&quot;, &quot;numDimensions&quot;: 1536, &quot;similarity&quot;: &quot;cosine&quot; }, { &quot;type&quot;: &quot;filter&quot;, &quot;path&quot;: &quot;year&quot; }, { &quot;type&quot;: &quot;filter&quot;, &quot;path&quot;: &quot;language&quot; } ] }   The only difference is that we've added this part, stating that year and language should be indexed as a filter.   { &quot;type&quot;: &quot;filter&quot;, &quot;path&quot;: &quot;year&quot; }, { &quot;type&quot;: &quot;filter&quot;, &quot;path&quot;: &quot;language&quot; }   Add that new aggregation pipeline in your code (server/src/controllers/books.ts inside the now familiar searchBooks method) and when searching, you'll get semantic results written in Spanish. ","version":"Next","tagName":"h2"},{"title":"üìò Semantic Search","type":0,"sectionRef":"#","url":"/search-lab/docs/vector-search/semantic-search","content":"","keywords":"","version":"Next"},{"title":"What is semantic search?‚Äã","type":1,"pageTitle":"üìò Semantic Search","url":"/search-lab/docs/vector-search/semantic-search#what-is-semantic-search","content":" Semantic search is a search technique that uses the meaning of words to find relevant results. It's what powers large language models that we see nowadays.  Using semantic search, we can find relevant results even if the search terms don't appear in the results. For example, if we search for &quot;How to make a cake&quot;, we can find results that contain the words &quot;How to bake a cake&quot; or &quot;How to make a pie&quot;.  This is done with vectors. Vectors are mathematical representations of words. They are used to find the similarity between words. For example, the word &quot;cake&quot; is similar to the word &quot;pie&quot; because they are both desserts.  ","version":"Next","tagName":"h2"},{"title":"How to create a semantic search engine‚Äã","type":1,"pageTitle":"üìò Semantic Search","url":"/search-lab/docs/vector-search/semantic-search#how-to-create-a-semantic-search-engine","content":" In our library application, let's change how our search bar works. Let's try to change the search bar behavior to find books based on their meaning, not just the words themselves.  This will help us find books such as &quot;The Stand&quot; when searching for &quot;plague apocalypse&quot;.  Even if those words don't appear in the book title or description, we want to be able to find them.  You could use something similar to provide your customers with items that are similar to the ones they are looking at. For example, if a customer is looking at a pair of hiking boots, you could show them other equipment they might need.  ","version":"Next","tagName":"h2"},{"title":"Where does MongoDB come in?‚Äã","type":1,"pageTitle":"üìò Semantic Search","url":"/search-lab/docs/vector-search/semantic-search#where-does-mongodb-come-in","content":" With its document model, MongoDB is a great fit for storing vectors. You can store vectors as arrays of numbers in a document.  When time comes to search for the relevant results, we can leverage the power of Lucene, just like we did for the full-text search. ","version":"Next","tagName":"h2"},{"title":"Using Google Cloud Vertex AI","type":0,"sectionRef":"#","url":"/search-lab/docs/vector-search/create-vectors/google-vertex","content":"","keywords":"","version":"Next"},{"title":"Create a Google Cloud account‚Äã","type":1,"pageTitle":"Using Google Cloud Vertex AI","url":"/search-lab/docs/vector-search/create-vectors/google-vertex#create-a-google-cloud-account","content":" The first step is to create a Google Cloud account. Use the following link to get started with some free credits.  Sign Up for a Google Cloud Account  ","version":"Next","tagName":"h2"},{"title":"Create new project‚Äã","type":1,"pageTitle":"Using Google Cloud Vertex AI","url":"/search-lab/docs/vector-search/create-vectors/google-vertex#create-new-project","content":" For the welcome screen, create a new project. You can name it whatever you like.  Make sure that you pick an active billing account.    Once the fields are filled out, click the Create button.  ","version":"Next","tagName":"h2"},{"title":"Open Cloud Shell‚Äã","type":1,"pageTitle":"Using Google Cloud Vertex AI","url":"/search-lab/docs/vector-search/create-vectors/google-vertex#open-cloud-shell","content":" Once your project is created, look for the Activate Cloud Shell button in the top right corner of the screen.    This will open up a terminal-like window in your browser. This is a fully functional terminal that is connected to a virtual machine in the cloud. You can use this terminal to run commands on your virtual machine.  ","version":"Next","tagName":"h2"},{"title":"Enable the AI Platform API‚Äã","type":1,"pageTitle":"Using Google Cloud Vertex AI","url":"/search-lab/docs/vector-search/create-vectors/google-vertex#enable-the-ai-platform-api","content":" You will need to enable the AI Platform API for your project. You can do this by running the following command in your Cloud Shell.  gcloud services enable aiplatform.googleapis.com   ","version":"Next","tagName":"h2"},{"title":"Create an authentication file‚Äã","type":1,"pageTitle":"Using Google Cloud Vertex AI","url":"/search-lab/docs/vector-search/create-vectors/google-vertex#create-an-authentication-file","content":" To authenticate with the AI Platform API, you will need to create a file that contains your application credentials. You can do this by running the following command in your Cloud Shell.  gcloud auth application-default login   Follow the instructions in the terminal to authenticate with your Google Cloud account. Once you have authenticated, you will see a message similar to the following.  Credentials saved to file: [/tmp/tmp.n0HdRFDDv8/application_default_credentials.json]   Save the credentials file to your home directory.  mv /tmp/tmp.n0HdRFDDv8/application_default_credentials.json ~/credentials.json   ","version":"Next","tagName":"h2"},{"title":"Create embeddings for some text‚Äã","type":1,"pageTitle":"Using Google Cloud Vertex AI","url":"/search-lab/docs/vector-search/create-vectors/google-vertex#create-embeddings-for-some-text","content":" To create embeddings for some text, you will start by creating a request.json file. This file will contain the text that you want to convert into embeddings. Run the following command in the Cloud Shell to create this file.  echo '{ &quot;instances&quot;: [ { &quot;text&quot;: &quot;picture of a cat&quot; } ] }' &gt;&gt; request.json   Now run the following curl command to get the embeddings for the text.  curl -X POST \\ -H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\ -H &quot;Content-Type: application/json; charset=utf-8&quot; \\ -d @request.json \\ &quot;https://us-central1-aiplatform.googleapis.com/v1/projects/$PROJECT_ID/locations/us-central1/publishers/google/models/multimodalembedding@001:predict&quot;   Make sure to change the $PROJECT_ID variable to your project id.  You will receive a response similar to  { &quot;predictions&quot;: [ { &quot;textEmbedding&quot;: [ -0.00566103263, 0.0202014241, -0.00677233562, 0.0180264488, 0.0265100803, ... 0.00116232142, 0.0134601779, -0.00257002981 ] } ], &quot;deployedModelId&quot;: &quot;5595742328217141248&quot; }   You will notice that the textEmbeddings fields contains an array of 1408 numbers. These are the embeddings for the text that you provided.  ","version":"Next","tagName":"h2"},{"title":"Create embeddings for the books‚Äã","type":1,"pageTitle":"Using Google Cloud Vertex AI","url":"/search-lab/docs/vector-search/create-vectors/google-vertex#create-embeddings-for-the-books","content":" To create the embeddings for the books in your collection, you should run this curl command for each book. This process is somewhat time consuming, so we've already created them for you.  You can find the 1408 dimensions vector in embeddings field of the new books collection you imported.  Because we already have the vectors for the books, we can use them with Vector Search.  ","version":"Next","tagName":"h2"},{"title":"Querying with vectors‚Äã","type":1,"pageTitle":"Using Google Cloud Vertex AI","url":"/search-lab/docs/vector-search/create-vectors/google-vertex#querying-with-vectors","content":" To query the data, Vector Search will need to calculate the distance between the query vector and the vectors of the documents in the collection.  To do so, you will need to vectorize your query. You can use the same function to vectorize your query as well.  In the library application, we've created a function that will vectorize your query for you. You can find it in the server/embeddings/googleVertex.mjs file.  import aiplatform from '@google-cloud/aiplatform'; const project = process.env.PROJECT_ID; const location = process.env.PROJECT_LOCATION; const {PredictionServiceClient} = aiplatform.v1; const {helpers} = aiplatform; const predictionServiceClient = new PredictionServiceClient({ apiEndpoint: 'us-central1-aiplatform.googleapis.com' }); const getTermEmbeddings = async (text) =&gt; { const publisher = &quot;google&quot;; const model = 'multimodalembedding@001'; // Configure the parent resource const endpoint = `projects/${project}/locations/${location}/publishers/${publisher}/models/${model}`; const instance = { text }; const instanceValue = helpers.toValue(instance); const instances = [instanceValue]; const request = { endpoint, instances }; // Predict request const [response] = await predictionServiceClient.predict(request); const embeddings = response.predictions[0].structValue.fields.textEmbedding.listValue.values.map(e =&gt; e.numberValue); return embeddings; }; export default getTermEmbeddings;   ","version":"Next","tagName":"h2"},{"title":"Configuring the application‚Äã","type":1,"pageTitle":"Using Google Cloud Vertex AI","url":"/search-lab/docs/vector-search/create-vectors/google-vertex#configuring-the-application","content":" In your server/.env file, you'll find a few variables that you can use to configure the application.  The first one is EMBEDDINGS_SOURCE. It tells the application where to get the embeddings from. You can set it to googleVertex.  Then set the EMBEDDING_KEY to your credentials.json file.  Finally, set the PROJECT_ID and PROJECT_LOCATION to the values for your project.  EMBEDDINGS_SOURCE=googleVertex EMBEDDING_KEY=&quot;./credentials.json&quot; PROJECT_ID=projectphoenix-verteximage PROJECT_LOCATION=us-central1   Your application now has a getTermEmbeddings function that will return the embeddings for a given text. You can see the details of this file in the server/src/embeddings/googleVertex.js file. ","version":"Next","tagName":"h2"},{"title":"üìò What Are Vectors?","type":0,"sectionRef":"#","url":"/search-lab/docs/vector-search/what-are-vector","content":"","keywords":"","version":"Next"},{"title":"Why Do We Need Vectors?‚Äã","type":1,"pageTitle":"üìò What Are Vectors?","url":"/search-lab/docs/vector-search/what-are-vector#why-do-we-need-vectors","content":" Computers can't understand text. They can only understand numbers. So, we need a way to convert text into numbers. That's where vectors come in.  Using vectors, we can plot text in a multi-dimensional space. It is hard to visualize a multi-dimensional space, so let's start with a 2-dimensional space.  Imagine a plot with a x and y axis. Our ML model will plot various points on this plot. This could represent words, sentences, paragraphs, documents, or even images.  The position where the points are plotted is determined by the model you used. The models converts the data you passed it into a vector. Then, it plots the vector on the chart.    When doing a search, we will create a new vector for the search term. We then plot this new vector on the chart.    Then, we will find the closest words to the search term. The closest words will be the words that are plotted closest to the search term.  The closest term will depend on the algorithm you use to calculate the distance between vectors. Using Euclidian distance, the closest words will be the words that are closest to the search term.    Vector search also provides a cosine algorithm. Using cosine distance, the closest words will be the words that are closest to the search term, but in the same direction.    ","version":"Next","tagName":"h2"},{"title":"How Do We Create Vectors?‚Äã","type":1,"pageTitle":"üìò What Are Vectors?","url":"/search-lab/docs/vector-search/what-are-vector#how-do-we-create-vectors","content":" The big breakthrough with GenAI is that developers can now easily use models that have been pre-trained, and made available freely online. These models have been trained on huge datasets, and are able to convert text (or any sort of data, really) into vectors.  There are many ways to create vectors. In this workshop, we'll use a pre-trained model and an API that will return vectors for us. ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}